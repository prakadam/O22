# ============================================================================
# DEBUG: STEP BY STEP DATA CHECK
# ============================================================================

import pandas as pd
import numpy as np
import json

MODEL_NAME = 'pre_ole'
DATA_DIR = '/dbfs/FileStore/ole_model/'

# ============================================================================
# STEP 1: LOAD AND COUNT
# ============================================================================

print("="*70)
print("STEP 1: LOAD DATA")
print("="*70)

train_df = pd.read_parquet(f'{DATA_DIR}{MODEL_NAME}_train.parquet')
val_df = pd.read_parquet(f'{DATA_DIR}{MODEL_NAME}_val.parquet')
test_df = pd.read_parquet(f'{DATA_DIR}{MODEL_NAME}_test.parquet')

print(f"Train: {len(train_df):,}")
print(f"Val: {len(val_df):,}")
print(f"Test: {len(test_df):,}")

full_df = pd.concat([train_df, val_df, test_df], ignore_index=True)
print(f"Total: {len(full_df):,}")

# ============================================================================
# STEP 2: CHECK COLUMNS
# ============================================================================

print("\n" + "="*70)
print("STEP 2: COLUMNS")
print("="*70)

print(f"Columns: {full_df.columns.tolist()}")

# ============================================================================
# STEP 3: CHECK SAMPLE ROW
# ============================================================================

print("\n" + "="*70)
print("STEP 3: SAMPLE ROW")
print("="*70)

sample = full_df.iloc[0]
print(f"page_ids type: {type(sample['page_ids'])}")
print(f"page_ids length: {len(sample['page_ids'])}")
print(f"page_ids first 10: {sample['page_ids'][:10]}")
print(f"page_ids non-zero count: {sum(1 for p in sample['page_ids'] if p != 0)}")

print(f"\ntime_deltas type: {type(sample['time_deltas'])}")
print(f"time_deltas length: {len(sample['time_deltas'])}")
print(f"time_deltas first 10: {sample['time_deltas'][:10]}")

print(f"\nlabel: {sample['label']}")
print(f"seq_len: {sample['seq_len']}")

# ============================================================================
# STEP 4: CHECK VOCAB
# ============================================================================

print("\n" + "="*70)
print("STEP 4: VOCABULARY")
print("="*70)

with open(f'{DATA_DIR}page_vocab_{MODEL_NAME}.json', 'r') as f:
    page_vocab = json.load(f)

print(f"Vocab size: {len(page_vocab)}")
print(f"Sample pages: {list(page_vocab.keys())[:10]}")

id_to_page = {v: k for k, v in page_vocab.items()}

# ============================================================================
# STEP 5: EXTRACT UNIQUE SETS
# ============================================================================

print("\n" + "="*70)
print("STEP 5: UNIQUE PAGE SETS")
print("="*70)

def get_page_set(row, id_to_page):
    """Get unique pages as frozenset"""
    page_ids = row['page_ids']
    pages = set()
    for pid in page_ids:
        if pid != 0:
            pages.add(id_to_page.get(pid, 'UNK'))
    return frozenset(pages)

# Get all unique sets
full_df['page_set'] = full_df.apply(lambda r: get_page_set(r, id_to_page), axis=1)

unique_sets = full_df['page_set'].nunique()
print(f"Unique page sets: {unique_sets:,}")

# ============================================================================
# STEP 6: SET SIZE DISTRIBUTION
# ============================================================================

print("\n" + "="*70)
print("STEP 6: SET SIZE DISTRIBUTION")
print("="*70)

full_df['set_size'] = full_df['page_set'].apply(len)

print(f"Min pages per user: {full_df['set_size'].min()}")
print(f"Max pages per user: {full_df['set_size'].max()}")
print(f"Median pages per user: {full_df['set_size'].median()}")
print(f"Mean pages per user: {full_df['set_size'].mean():.1f}")

print("\nDistribution:")
print(full_df['set_size'].describe())

# ============================================================================
# STEP 7: LABEL DISTRIBUTION
# ============================================================================

print("\n" + "="*70)
print("STEP 7: LABEL DISTRIBUTION")
print("="*70)

print(full_df['label'].value_counts())
print(f"\nConversion rate: {full_df['label'].mean()*100:.1f}%")

# ============================================================================
# STEP 8: TIME DISTRIBUTION
# ============================================================================

print("\n" + "="*70)
print("STEP 8: TIME DISTRIBUTION")
print("="*70)

full_df['total_time'] = full_df['time_deltas'].apply(lambda x: sum(x) if x is not None else 0)

print(f"Min total time: {full_df['total_time'].min()/60:.1f} min")
print(f"Max total time: {full_df['total_time'].max()/60:.1f} min")
print(f"Median total time: {full_df['total_time'].median()/60:.1f} min")
print(f"Mean total time: {full_df['total_time'].mean()/60:.1f} min")

print(f"\nUsers with 0 time: {(full_df['total_time'] == 0).sum():,}")
print(f"Users with <1 min: {(full_df['total_time'] < 60).sum():,}")
print(f"Users with <5 min: {(full_df['total_time'] < 300).sum():,}")

# ============================================================================
# SUMMARY
# ============================================================================

print("\n" + "="*70)
print("SUMMARY")
print("="*70)

print(f"""
Total Users: {len(full_df):,}
Unique Page Sets: {unique_sets:,}
Unique Pages (Vocab): {len(page_vocab)}

Pages per User:
  Min: {full_df['set_size'].min()}
  Median: {full_df['set_size'].median():.0f}
  Max: {full_df['set_size'].max()}

Time per User:
  Min: {full_df['total_time'].min()/60:.1f} min
  Median: {full_df['total_time'].median()/60:.1f} min
  Max: {full_df['total_time'].max()/60:.1f} min

Labels:
  Positive: {full_df['label'].sum():,}
  Negative: {(1-full_df['label']).sum():,}
  Conversion: {full_df['label'].mean()*100:.1f}%
""")


# ============================================================================
# CHECK SET FREQUENCY (How many users per set)
# ============================================================================

from collections import Counter

# Count users per set
set_counts = Counter(full_df['page_set'].tolist())

print("="*70)
print("SET FREQUENCY DISTRIBUTION")
print("="*70)

print(f"Total unique sets: {len(set_counts):,}")

# Distribution
freq_dist = Counter(set_counts.values())
print(f"\nSets with 1 user: {freq_dist[1]:,}")
print(f"Sets with 2-5 users: {sum(v for k,v in freq_dist.items() if 2<=k<=5):,}")
print(f"Sets with 6-10 users: {sum(v for k,v in freq_dist.items() if 6<=k<=10):,}")
print(f"Sets with 11-20 users: {sum(v for k,v in freq_dist.items() if 11<=k<=20):,}")
print(f"Sets with 20+ users: {sum(v for k,v in freq_dist.items() if k>20):,}")

# Top sets
print(f"\nTop 10 most common sets:")
for page_set, count in set_counts.most_common(10):
    pages = sorted(list(page_set))
    print(f"  {count:>5} users | {len(pages):>2} pages | {', '.join(pages[:5])}{'...' if len(pages)>5 else ''}")

# Sets with MIN_USERS (20)
sets_with_min = sum(1 for c in set_counts.values() if c >= 20)
print(f"\nSets with 20+ users: {sets_with_min:,}")


# ============================================================================
# BETTER APPROACH: PAGE PRESENCE MATRIX
# ============================================================================

print("="*70)
print("PAGE PRESENCE ANALYSIS (Binary: visited or not)")
print("="*70)

# Create binary presence matrix (user × page)
all_pages = sorted(list(id_to_page.values()))
print(f"Total unique pages: {len(all_pages)}")

# For each page, count users who visited
page_presence = {}
for page in all_pages:
    has_page = full_df['page_set'].apply(lambda s: page in s)
    
    pos_has = has_page[full_df['label'] == 1].sum()
    neg_has = has_page[full_df['label'] == 0].sum()
    
    total_pos = full_df['label'].sum()
    total_neg = (1 - full_df['label']).sum()
    
    page_presence[page] = {
        'page': page,
        'pos_users': pos_has,
        'neg_users': neg_has,
        'pos_pct': round(pos_has / total_pos * 100, 1),
        'neg_pct': round(neg_has / total_neg * 100, 1),
        'lift': round((pos_has/total_pos) / (neg_has/total_neg), 2) if neg_has > 0 else 5.0,
        'total_users': pos_has + neg_has
    }

# Convert to dataframe
presence_df = pd.DataFrame(page_presence.values())
presence_df = presence_df.sort_values('lift', ascending=False)

print(f"\n{'Page':<40} | {'Pos %':<8} | {'Neg %':<8} | {'Lift':<8} | Users")
print("-"*80)

print("\nTOP 15 POSITIVE SIGNALS (High Lift):")
for _, r in presence_df.head(15).iterrows():
    print(f"{r['page'][:40]:<40} | {r['pos_pct']:<8} | {r['neg_pct']:<8} | {r['lift']:<8}x | {r['total_users']}")

print("\nTOP 15 NEGATIVE SIGNALS (Low Lift):")
for _, r in presence_df.tail(15).iterrows():
    print(f"{r['page'][:40]:<40} | {r['pos_pct']:<8} | {r['neg_pct']:<8} | {r['lift']:<8}x | {r['total_users']}")

presence_df.to_csv(f'{DATA_DIR}page_presence_{MODEL_NAME}.csv', index=False)
print(f"\n✓ Saved: page_presence_{MODEL_NAME}.csv ({len(presence_df)} pages)")

# ============================================================================
# SET SIZE ANALYSIS (Instead of exact sets)
# ============================================================================

print("\n" + "="*70)
print("SET SIZE ANALYSIS (How many pages visited)")
print("="*70)

full_df['set_size'] = full_df['page_set'].apply(len)

# Group by set size
size_stats = full_df.groupby('set_size').agg({
    'label': ['sum', 'count', 'mean'],
    'total_time': 'median'
}).reset_index()
size_stats.columns = ['set_size', 'conversions', 'users', 'conv_rate', 'median_time']
size_stats['conv_pct'] = (size_stats['conv_rate'] * 100).round(1)
size_stats['median_time_min'] = (size_stats['median_time'] / 60).round(1)

print(f"\n{'Pages':<8} | {'Users':<10} | {'Conversions':<12} | {'Conv %':<8} | {'Med Time'}")
print("-"*60)
for _, r in size_stats.head(30).iterrows():
    print(f"{r['set_size']:<8} | {r['users']:<10} | {r['conversions']:<12.0f} | {r['conv_pct']:<8} | {r['median_time_min']} min")

size_stats.to_csv(f'{DATA_DIR}set_size_analysis_{MODEL_NAME}.csv', index=False)
print(f"\n✓ Saved: set_size_analysis_{MODEL_NAME}.csv")
