# ============================================================================
# CLEAN JOURNEY ANALYSIS — MEANINGFUL DATA ONLY
# ============================================================================

import pandas as pd
import numpy as np
from collections import Counter
from prefixspan import PrefixSpan

MODEL_NAME = 'pre_ole'
DATA_DIR = '/dbfs/FileStore/ole_model/'

# ============================================================================
# LOAD FULL DATA (not just test)
# ============================================================================

# Load all splits
train_df = pd.read_parquet(f'{DATA_DIR}{MODEL_NAME}_train.parquet')
val_df = pd.read_parquet(f'{DATA_DIR}{MODEL_NAME}_val.parquet')
test_df = pd.read_parquet(f'{DATA_DIR}{MODEL_NAME}_test.parquet')

# Combine
full_df = pd.concat([train_df, val_df, test_df], ignore_index=True)

with open(f'{DATA_DIR}page_vocab_{MODEL_NAME}.json', 'r') as f:
    page_vocab = json.load(f)
id_to_page = {v: k for k, v in page_vocab.items()}

print(f"Full dataset: {len(full_df):,} users")

# ============================================================================
# EXTRACT JOURNEYS WITH TIME FILTERING
# ============================================================================

MIN_TIME_PER_PAGE = 10    # Minimum 10 seconds (filter noise)
MIN_JOURNEY_LENGTH = 10   # At least 10 pages
MIN_TOTAL_TIME = 60       # At least 1 minute total journey

def extract_journey_clean(row, id_to_page):
    """Extract journey with quality filters"""
    page_ids = row['page_ids']
    time_deltas = row['time_deltas']
    
    if page_ids is None or len(page_ids) == 0:
        return [], [], 0
    
    page_times = {}
    page_first_pos = {}
    
    for i, (pid, time) in enumerate(zip(page_ids, time_deltas)):
        if pid == 0:
            continue
        
        page = id_to_page.get(pid, 'UNK')
        time_val = float(time or 0)
        
        if page in page_times:
            page_times[page] += time_val
        else:
            page_times[page] = time_val
            page_first_pos[page] = i
    
    # Sort by first occurrence
    sorted_pages = sorted(page_times.keys(), key=lambda p: page_first_pos[p])
    times = [page_times[p] for p in sorted_pages]
    total_time = sum(times)
    
    return sorted_pages, times, total_time

# Apply
full_df['pages'], full_df['times'], full_df['total_time'] = zip(
    *full_df.apply(lambda row: extract_journey_clean(row, id_to_page), axis=1)
)
full_df['journey_length'] = full_df['pages'].apply(len)

print(f"Extracted journeys")

# ============================================================================
# FILTER: TOP 50% QUALITY JOURNEYS
# ============================================================================

# Calculate percentiles
time_p50 = full_df['total_time'].quantile(0.50)
time_p90 = full_df['total_time'].quantile(0.90)
len_p50 = full_df['journey_length'].quantile(0.50)

print(f"\nJourney Stats (All Users):")
print(f"  Time P50: {time_p50/60:.1f} min")
print(f"  Time P90: {time_p90/60:.1f} min")
print(f"  Length P50: {len_p50:.0f} pages")

# Filter criteria for quality journeys:
# 1. Total time >= 1 minute
# 2. Journey length >= 10 pages
# 3. Total time <= P90 (remove outliers)

quality_df = full_df[
    (full_df['total_time'] >= MIN_TOTAL_TIME) &
    (full_df['journey_length'] >= MIN_JOURNEY_LENGTH) &
    (full_df['total_time'] <= time_p90)
].copy()

print(f"\nQuality Journeys: {len(quality_df):,} ({len(quality_df)/len(full_df)*100:.0f}% of total)")

# Split by outcome
pos_df = quality_df[quality_df['label'] == 1]
neg_df = quality_df[quality_df['label'] == 0]

print(f"  Positive (completers): {len(pos_df):,}")
print(f"  Negative (non-completers): {len(neg_df):,}")

# ============================================================================
# TIME CLEANING: Use Median, Remove Low Times
# ============================================================================

def clean_times(pages, times, min_time=10):
    """Replace low times with median of valid times"""
    valid_times = [t for t in times if t >= min_time]
    
    if not valid_times:
        return times  # Can't fix
    
    median_time = np.median(valid_times)
    
    # Replace low times with median
    cleaned = [t if t >= min_time else median_time for t in times]
    return cleaned

pos_df['times_clean'] = pos_df.apply(lambda r: clean_times(r['pages'], r['times']), axis=1)
neg_df['times_clean'] = neg_df.apply(lambda r: clean_times(r['pages'], r['times']), axis=1)

# ============================================================================
# PREFIXSPAN WITH QUALITY DATA
# ============================================================================

def get_quality_patterns(df, min_support_pct=0.01, min_len=10, max_len=50):
    """Extract patterns from quality journeys only"""
    
    sequences = df['pages'].tolist()
    times_list = df['times_clean'].tolist()
    
    # Filter by minimum length
    valid = [(s, t) for s, t in zip(sequences, times_list) if len(s) >= min_len]
    sequences = [v[0] for v in valid]
    times_list = [v[1] for v in valid]
    
    if len(sequences) < 10:
        print("Not enough sequences")
        return []
    
    print(f"Mining {len(sequences)} sequences...")
    
    ps = PrefixSpan(sequences)
    ps.minlen = min_len
    ps.maxlen = max_len
    
    min_support = max(int(len(sequences) * min_support_pct), 5)
    patterns = ps.frequent(min_support)
    
    print(f"Found {len(patterns)} raw patterns")
    
    # Calculate stats with cleaned times
    pattern_stats = []
    for support, pattern in patterns:
        if len(pattern) < min_len:
            continue
            
        matching_times = []
        for seq, times in zip(sequences, times_list):
            try:
                # Find pattern in sequence
                pat_str = '→'.join(pattern)
                seq_str = '→'.join(seq)
                if pat_str in seq_str:
                    idx = seq.index(pattern[0])
                    end = min(idx + len(pattern), len(times))
                    total = sum(times[idx:end])
                    if total > 0:  # Only valid times
                        matching_times.append(total)
            except:
                continue
        
        if matching_times and len(matching_times) >= 3:
            pattern_stats.append({
                'pattern': ' → '.join(pattern),
                'length': len(pattern),
                'users': support,
                'avg_time_min': round(np.mean(matching_times) / 60, 1),
                'median_time_min': round(np.median(matching_times) / 60, 1),
                'time_per_step': round(np.mean(matching_times) / len(pattern) / 60, 2),
                'p10_time': round(np.percentile(matching_times, 10) / 60, 1),
                'p90_time': round(np.percentile(matching_times, 90) / 60, 1)
            })
    
    # Sort by length (longer = more meaningful), then users
    return sorted(pattern_stats, key=lambda x: (-x['length'], -x['users']))

print("\n" + "="*70)
print("EXTRACTING QUALITY PATTERNS")
print("="*70)

pos_patterns = get_quality_patterns(pos_df, min_support_pct=0.01, min_len=10, max_len=50)
neg_patterns = get_quality_patterns(neg_df, min_support_pct=0.01, min_len=10, max_len=50)

print(f"\nQuality Positive Patterns: {len(pos_patterns)}")
print(f"Quality Negative Patterns: {len(neg_patterns)}")

# ============================================================================
# DISPLAY MEANINGFUL RESULTS
# ============================================================================

print("\n" + "="*70)
print("TOP 10 SUCCESSFUL JOURNEYS (Quality Data)")
print("="*70)
print(f"{'Length':<6} | {'Users':<6} | {'Median Time':<12} | {'Per Step':<10} | Pattern")
print("-"*100)

for p in pos_patterns[:10]:
    print(f"{p['length']:<6} | {p['users']:<6} | {p['median_time_min']:<10} min | {p['time_per_step']:<8} min | {p['pattern'][:50]}...")

print("\n" + "="*70)
print("TOP 10 FAILED JOURNEYS (Quality Data)")
print("="*70)
print(f"{'Length':<6} | {'Users':<6} | {'Median Time':<12} | {'Per Step':<10} | Pattern")
print("-"*100)

for p in neg_patterns[:10]:
    print(f"{p['length']:<6} | {p['users']:<6} | {p['median_time_min']:<10} min | {p['time_per_step']:<8} min | {p['pattern'][:50]}...")

# ============================================================================
# SAVE
# ============================================================================

pd.DataFrame(pos_patterns).to_csv(f'{DATA_DIR}quality_successful_journeys_{MODEL_NAME}.csv', index=False)
pd.DataFrame(neg_patterns).to_csv(f'{DATA_DIR}quality_failed_journeys_{MODEL_NAME}.csv', index=False)

print(f"\n✓ Saved: quality_successful_journeys_{MODEL_NAME}.csv")
print(f"✓ Saved: quality_failed_journeys_{MODEL_NAME}.csv")
```

---

## What Changed

| Before | After |
|--------|-------|
| Test data only | Full dataset (train + val + test) |
| All journeys | Top 50% quality (filtered) |
| Raw times (0.0 min) | Cleaned times (median replacement) |
| Any length | Minimum 10 pages |
| Avg time | Median time (more robust) |
| Short patterns | Minimum 10-step patterns |

---

## Quality Filters Applied
```
1. Total journey time >= 1 minute
2. Journey length >= 10 pages
3. Total time <= 90th percentile (remove outliers)
4. Per-page time >= 10 seconds (or replace with median)
5. Pattern length >= 10 steps


# ============================================================================
# CHECK SET FREQUENCY (With Time, Sequence Length & Conversion)
# ============================================================================
from collections import Counter, defaultdict
import numpy as np

# Calculate fields if not already done
if 'total_time' not in full_df.columns:
    full_df['total_time'] = full_df['time_deltas'].apply(lambda x: sum(x) if x is not None else 0)

if 'actual_seq_len' not in full_df.columns:
    full_df['actual_seq_len'] = full_df['page_ids'].apply(lambda x: sum(1 for p in x if p != 0))

# Count users per set + aggregate stats
set_stats = defaultdict(lambda: {'count': 0, 'times': [], 'seq_lens': [], 'labels': []})

for _, row in full_df.iterrows():
    key = row['page_set']
    set_stats[key]['count'] += 1
    set_stats[key]['times'].append(row['total_time'])
    set_stats[key]['seq_lens'].append(row['actual_seq_len'])
    set_stats[key]['labels'].append(row['label'])

# Overall conversion rate (for lift calculation)
overall_conv = full_df['label'].mean()

print("="*70)
print("SET FREQUENCY DISTRIBUTION (With Time, SeqLen & Conversion)")
print("="*70)

print(f"Total unique sets: {len(set_stats):,}")
print(f"Overall conversion rate: {overall_conv*100:.1f}%")

# Distribution
freq_dist = Counter(s['count'] for s in set_stats.values())
print(f"\nSets with 1 user: {freq_dist[1]:,}")
print(f"Sets with 2-5 users: {sum(v for k,v in freq_dist.items() if 2<=k<=5):,}")
print(f"Sets with 6-10 users: {sum(v for k,v in freq_dist.items() if 6<=k<=10):,}")
print(f"Sets with 11-20 users: {sum(v for k,v in freq_dist.items() if 11<=k<=20):,}")
print(f"Sets with 20+ users: {sum(v for k,v in freq_dist.items() if k>20):,}")

# Top sets
print(f"\nTop 15 most common sets:")
print(f"{'Users':>6} | {'Pages':>5} | {'Time':>8} | {'SeqLen':>7} | {'Conv%':>6} | {'Lift':>5} | Pages")
print("-"*100)

top_sets = sorted(set_stats.items(), key=lambda x: -x[1]['count'])[:15]
for page_set, stats in top_sets:
    pages = sorted(list(page_set))
    med_time = np.median(stats['times']) / 60
    med_seq = np.median(stats['seq_lens'])
    conv_rate = np.mean(stats['labels'])
    lift = conv_rate / overall_conv if overall_conv > 0 else 0
    
    print(f"{stats['count']:>6} | {len(pages):>5} | {med_time:>6.1f}m | {med_seq:>7.0f} | {conv_rate*100:>5.1f}% | {lift:>5.2f}x | {', '.join(pages[:3])}{'...' if len(pages)>3 else ''}")

# High converting sets (20+ users)
print(f"\n" + "="*70)
print("TOP 15 HIGH CONVERTING SETS (20+ users, sorted by lift)")
print("="*70)
print(f"{'Users':>6} | {'Pages':>5} | {'Time':>8} | {'SeqLen':>7} | {'Conv%':>6} | {'Lift':>5} | Pages")
print("-"*100)

# Filter and sort by conversion
high_conv_sets = [(k, v) for k, v in set_stats.items() if v['count'] >= 20]
high_conv_sets = sorted(high_conv_sets, key=lambda x: -np.mean(x[1]['labels']))[:15]

for page_set, stats in high_conv_sets:
    pages = sorted(list(page_set))
    med_time = np.median(stats['times']) / 60
    med_seq = np.median(stats['seq_lens'])
    conv_rate = np.mean(stats['labels'])
    lift = conv_rate / overall_conv if overall_conv > 0 else 0
    
    print(f"{stats['count']:>6} | {len(pages):>5} | {med_time:>6.1f}m | {med_seq:>7.0f} | {conv_rate*100:>5.1f}% | {lift:>5.2f}x | {', '.join(pages[:3])}{'...' if len(pages)>3 else ''}")

# Low converting sets (dropout signals)
print(f"\n" + "="*70)
print("TOP 15 LOW CONVERTING SETS (20+ users, dropout signals)")
print("="*70)
print(f"{'Users':>6} | {'Pages':>5} | {'Time':>8} | {'SeqLen':>7} | {'Conv%':>6} | {'Lift':>5} | Pages")
print("-"*100)

low_conv_sets = [(k, v) for k, v in set_stats.items() if v['count'] >= 20]
low_conv_sets = sorted(low_conv_sets, key=lambda x: np.mean(x[1]['labels']))[:15]

for page_set, stats in low_conv_sets:
    pages = sorted(list(page_set))
    med_time = np.median(stats['times']) / 60
    med_seq = np.median(stats['seq_lens'])
    conv_rate = np.mean(stats['labels'])
    lift = conv_rate / overall_conv if overall_conv > 0 else 0
    
    print(f"{stats['count']:>6} | {len(pages):>5} | {med_time:>6.1f}m | {med_seq:>7.0f} | {conv_rate*100:>5.1f}% | {lift:>5.2f}x | {', '.join(pages[:3])}{'...' if len(pages)>3 else ''}")

# Save to CSV
print(f"\n" + "="*70)
print("SAVING RESULTS")
print("="*70)

results = []
for page_set, stats in set_stats.items():
    if stats['count'] >= 5:  # At least 5 users
        pages = sorted(list(page_set))
        conv_rate = np.mean(stats['labels'])
        results.append({
            'page_set': ' | '.join(pages),
            'num_pages': len(pages),
            'users': stats['count'],
            'median_time_min': round(np.median(stats['times']) / 60, 1),
            'median_seq_len': int(np.median(stats['seq_lens'])),
            'conversions': sum(stats['labels']),
            'conv_rate_pct': round(conv_rate * 100, 1),
            'lift': round(conv_rate / overall_conv, 2) if overall_conv > 0 else 0
        })

results_df = pd.DataFrame(results).sort_values('lift', ascending=False)
results_df.to_csv(f'{DATA_DIR}set_analysis_{MODEL_NAME}.csv', index=False)

print(f"✓ Saved: set_analysis_{MODEL_NAME}.csv ({len(results_df)} sets with 5+ users)")



# ============================================================================
# DEBUG: TIME CALCULATION
# ============================================================================

print("="*70)
print("DEBUG: TIME VALUES")
print("="*70)

# Check raw time_deltas
sample = full_df.iloc[0]
print(f"Sample time_deltas type: {type(sample['time_deltas'])}")
print(f"Sample time_deltas length: {len(sample['time_deltas'])}")
print(f"Sample time_deltas first 10: {sample['time_deltas'][:10]}")
print(f"Sample time_deltas sum: {sum(sample['time_deltas'])}")

# Check distribution
print(f"\n--- TOTAL TIME DISTRIBUTION ---")
print(f"Min: {full_df['total_time'].min()}")
print(f"Max: {full_df['total_time'].max()}")
print(f"Median: {full_df['total_time'].median()}")
print(f"Mean: {full_df['total_time'].mean()}")

# Check units — is it seconds, milliseconds, or already minutes?
print(f"\n--- LIKELY UNIT ---")
print(f"If seconds: Median = {full_df['total_time'].median()/60:.1f} min")
print(f"If milliseconds: Median = {full_df['total_time'].median()/60000:.1f} min")
print(f"If already minutes: Median = {full_df['total_time'].median():.1f} min")

# Check zeros
print(f"\n--- ZERO/NULL CHECK ---")
print(f"Users with total_time = 0: {(full_df['total_time'] == 0).sum():,}")
print(f"Users with total_time < 1: {(full_df['total_time'] < 1).sum():,}")
print(f"Users with total_time < 60: {(full_df['total_time'] < 60).sum():,}")

# Check a few rows
print(f"\n--- SAMPLE ROWS ---")
for i in [0, 100, 1000]:
    row = full_df.iloc[i]
    times = row['time_deltas']
    non_zero = [t for t in times if t > 0]
    print(f"Row {i}: sum={sum(times):.2f}, non_zero={len(non_zero)}, max={max(times) if times else 0:.2f}")
